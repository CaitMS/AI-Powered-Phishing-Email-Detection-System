{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4be8abe",
   "metadata": {},
   "source": [
    "# AI-Powered Phishing Email Detection System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac324c0",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Phishing attacks are among the most prevalent forms of cyber threats, often relying on deceptive emails to trick recipients into revealing sensitive information or clicking malicious links. Traditional rule-based systems for phishing detection struggle to adapt to the rapidly evolving language and structure of phishing emails. As a result, artificial intelligence (AI) methodsâ€”particularly machine learningâ€”have become essential tools for building more flexible and accurate detection systems.\n",
    "\n",
    "In this project, we develop an AI-powered phishing email detection system that classifies emails as phishing or legitimate using natural language features and metadata. Our focus is on building a lightweight, interpretable prototype using the XGBoost classifier, a gradient boosting algorithm known for its performance and efficiency.\n",
    "\n",
    "We use a publicly available dataset from Kaggle that includes both phishing and legitimate emails with labeled examples. You can access the dataset here:\n",
    "\n",
    "[ðŸ”— Phishing Email Dataset on Kaggle](https://www.kaggle.com/datasets/naserabdullahalam/phishing-email-dataset)\n",
    "\n",
    "The project involves the following core components:\n",
    "*   Data cleaning and feature extraction from email content.\n",
    "*   Training and evaluation of an XGBoost classification model.\n",
    "*   Applying explainability techniques (e.g., SHAP) to interpret model predictions.\n",
    "*   Testing the model on real-world-like examples and documenting its strengths and limitations.\n",
    "\n",
    "The goal is to create a simple, explainable, and effective prototype that could form the basis of a real-world email threat detection tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83516cff",
   "metadata": {},
   "source": [
    "## Step 1: Environment set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc52b9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1ed3fb",
   "metadata": {},
   "source": [
    "## Step 2: Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a4cce34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       text_combined  label\n",
      "0  hpl nom may 25 2001 see attached file hplno 52...      0\n",
      "1  nom actual vols 24 th forwarded sabrae zajac h...      0\n",
      "2  enron actuals march 30 april 1 201 estimated a...      0\n",
      "3  hpl nom may 30 2001 see attached file hplno 53...      0\n",
      "4  hpl nom june 1 2001 see attached file hplno 60...      0\n",
      "Dataset shape: (82486, 2)\n",
      "Class distribution:\n",
      "label\n",
      "1    42891\n",
      "0    39595\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Where 'text_combined' contains the text data and 'label' is 1 for phishing, 0 for legitimate\n",
    "\n",
    "df = pd.read_csv('../dataset/phishing_email.csv')\n",
    "\n",
    "# Check the dataset\n",
    "print(df.head())\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Class distribution:\\n{df['label'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793367b0",
   "metadata": {},
   "source": [
    "## Step 3: Split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee1c58e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 65988\n",
      "Testing set size: 16498\n"
     ]
    }
   ],
   "source": [
    "X = df['text_combined']\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Testing set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf1f8d4",
   "metadata": {},
   "source": [
    "## Step 4: Create TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4538394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF feature matrix shape: (65988, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,  # Limit features to avoid dimensionality issues\n",
    "    min_df=5,           # Ignore terms that appear in less than 5 documents\n",
    "    max_df=0.7,         # Ignore terms that appear in more than 70% of documents\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2)  # Use both unigrams and bigrams\n",
    ")\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using the fitted vectorizer\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"TF-IDF feature matrix shape: {X_train_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc7e708",
   "metadata": {},
   "source": [
    "## Step 5: Train the XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bf19336",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caity\\OneDrive - University of Pretoria\\Documents\\COS 720\\Project\\venv\\Lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [20:15:45] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.64268\n",
      "[1]\tvalidation_0-logloss:0.60204\n",
      "[2]\tvalidation_0-logloss:0.56781\n",
      "[3]\tvalidation_0-logloss:0.53883\n",
      "[4]\tvalidation_0-logloss:0.51243\n",
      "[5]\tvalidation_0-logloss:0.48954\n",
      "[6]\tvalidation_0-logloss:0.46951\n",
      "[7]\tvalidation_0-logloss:0.44971\n",
      "[8]\tvalidation_0-logloss:0.43335\n",
      "[9]\tvalidation_0-logloss:0.41757\n",
      "[10]\tvalidation_0-logloss:0.40399\n",
      "[11]\tvalidation_0-logloss:0.39093\n",
      "[12]\tvalidation_0-logloss:0.37736\n",
      "[13]\tvalidation_0-logloss:0.36701\n",
      "[14]\tvalidation_0-logloss:0.35537\n",
      "[15]\tvalidation_0-logloss:0.34548\n",
      "[16]\tvalidation_0-logloss:0.33601\n",
      "[17]\tvalidation_0-logloss:0.32847\n",
      "[18]\tvalidation_0-logloss:0.32165\n",
      "[19]\tvalidation_0-logloss:0.31340\n",
      "[20]\tvalidation_0-logloss:0.30582\n",
      "[21]\tvalidation_0-logloss:0.29925\n",
      "[22]\tvalidation_0-logloss:0.29289\n",
      "[23]\tvalidation_0-logloss:0.28735\n",
      "[24]\tvalidation_0-logloss:0.28266\n",
      "[25]\tvalidation_0-logloss:0.27542\n",
      "[26]\tvalidation_0-logloss:0.27094\n",
      "[27]\tvalidation_0-logloss:0.26418\n",
      "[28]\tvalidation_0-logloss:0.25935\n",
      "[29]\tvalidation_0-logloss:0.25550\n",
      "[30]\tvalidation_0-logloss:0.25122\n",
      "[31]\tvalidation_0-logloss:0.24673\n",
      "[32]\tvalidation_0-logloss:0.24297\n",
      "[33]\tvalidation_0-logloss:0.23989\n",
      "[34]\tvalidation_0-logloss:0.23667\n",
      "[35]\tvalidation_0-logloss:0.23348\n",
      "[36]\tvalidation_0-logloss:0.22886\n",
      "[37]\tvalidation_0-logloss:0.22587\n",
      "[38]\tvalidation_0-logloss:0.22310\n",
      "[39]\tvalidation_0-logloss:0.22070\n",
      "[40]\tvalidation_0-logloss:0.21792\n",
      "[41]\tvalidation_0-logloss:0.21382\n",
      "[42]\tvalidation_0-logloss:0.21138\n",
      "[43]\tvalidation_0-logloss:0.20887\n",
      "[44]\tvalidation_0-logloss:0.20617\n",
      "[45]\tvalidation_0-logloss:0.20365\n",
      "[46]\tvalidation_0-logloss:0.20141\n",
      "[47]\tvalidation_0-logloss:0.19933\n",
      "[48]\tvalidation_0-logloss:0.19718\n",
      "[49]\tvalidation_0-logloss:0.19509\n",
      "[50]\tvalidation_0-logloss:0.19305\n",
      "[51]\tvalidation_0-logloss:0.19111\n",
      "[52]\tvalidation_0-logloss:0.18903\n",
      "[53]\tvalidation_0-logloss:0.18740\n",
      "[54]\tvalidation_0-logloss:0.18560\n",
      "[55]\tvalidation_0-logloss:0.18398\n",
      "[56]\tvalidation_0-logloss:0.18211\n",
      "[57]\tvalidation_0-logloss:0.18051\n",
      "[58]\tvalidation_0-logloss:0.17898\n",
      "[59]\tvalidation_0-logloss:0.17741\n",
      "[60]\tvalidation_0-logloss:0.17550\n",
      "[61]\tvalidation_0-logloss:0.17432\n",
      "[62]\tvalidation_0-logloss:0.17274\n",
      "[63]\tvalidation_0-logloss:0.17146\n",
      "[64]\tvalidation_0-logloss:0.17011\n",
      "[65]\tvalidation_0-logloss:0.16887\n",
      "[66]\tvalidation_0-logloss:0.16758\n",
      "[67]\tvalidation_0-logloss:0.16620\n",
      "[68]\tvalidation_0-logloss:0.16506\n",
      "[69]\tvalidation_0-logloss:0.16381\n",
      "[70]\tvalidation_0-logloss:0.16256\n",
      "[71]\tvalidation_0-logloss:0.16130\n",
      "[72]\tvalidation_0-logloss:0.16021\n",
      "[73]\tvalidation_0-logloss:0.15919\n",
      "[74]\tvalidation_0-logloss:0.15817\n",
      "[75]\tvalidation_0-logloss:0.15664\n",
      "[76]\tvalidation_0-logloss:0.15549\n",
      "[77]\tvalidation_0-logloss:0.15406\n",
      "[78]\tvalidation_0-logloss:0.15287\n",
      "[79]\tvalidation_0-logloss:0.15174\n",
      "[80]\tvalidation_0-logloss:0.15070\n",
      "[81]\tvalidation_0-logloss:0.14971\n",
      "[82]\tvalidation_0-logloss:0.14887\n",
      "[83]\tvalidation_0-logloss:0.14786\n",
      "[84]\tvalidation_0-logloss:0.14690\n",
      "[85]\tvalidation_0-logloss:0.14603\n",
      "[86]\tvalidation_0-logloss:0.14517\n",
      "[87]\tvalidation_0-logloss:0.14434\n",
      "[88]\tvalidation_0-logloss:0.14347\n",
      "[89]\tvalidation_0-logloss:0.14273\n",
      "[90]\tvalidation_0-logloss:0.14193\n",
      "[91]\tvalidation_0-logloss:0.14108\n",
      "[92]\tvalidation_0-logloss:0.14037\n",
      "[93]\tvalidation_0-logloss:0.13920\n",
      "[94]\tvalidation_0-logloss:0.13821\n",
      "[95]\tvalidation_0-logloss:0.13743\n",
      "[96]\tvalidation_0-logloss:0.13672\n",
      "[97]\tvalidation_0-logloss:0.13585\n",
      "[98]\tvalidation_0-logloss:0.13517\n",
      "[99]\tvalidation_0-logloss:0.13456\n",
      "Model training completed!\n"
     ]
    }
   ],
   "source": [
    "# Initialize XGBoost classifier\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    objective='binary:logistic',\n",
    "    use_label_encoder=False,\n",
    "    random_state=42,\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(\n",
    "    X_train_tfidf, \n",
    "    y_train,\n",
    "    eval_set=[(X_test_tfidf, y_test)]\n",
    ")\n",
    "\n",
    "print(\"Model training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9509d9c",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4893f4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9599\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96      7919\n",
      "           1       0.94      0.98      0.96      8579\n",
      "\n",
      "    accuracy                           0.96     16498\n",
      "   macro avg       0.96      0.96      0.96     16498\n",
      "weighted avg       0.96      0.96      0.96     16498\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[7413  506]\n",
      " [ 155 8424]]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred = xgb_model.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Print detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd49d0d7",
   "metadata": {},
   "source": [
    "## Step 7: Check feature importance (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a41e919c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 most important features:\n",
      "                                              Feature  Importance\n",
      "4959                                            wrote    0.035769\n",
      "1571                                            enron    0.026712\n",
      "820                                                cc    0.026674\n",
      "242   _______________________________________________    0.017291\n",
      "1222                                             date    0.016642\n",
      "2619                                              lar    0.016331\n",
      "1798                                             file    0.016046\n",
      "1000                                          company    0.013923\n",
      "560                                          aug 2008    0.013609\n",
      "2703                                             list    0.013142\n",
      "4692                                       university    0.012088\n",
      "4847                                          watches    0.011482\n",
      "3447                                         pleasure    0.011041\n",
      "119                                              2001    0.010658\n",
      "3003                                            money    0.010386\n",
      "940                                               cnn    0.010356\n",
      "2157                                             http    0.009782\n",
      "535                                          attached    0.009712\n",
      "3450                                               pm    0.009277\n",
      "2802                                             mail    0.009152\n"
     ]
    }
   ],
   "source": [
    "# Get feature importance\n",
    "feature_importance = xgb_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to better visualize feature importance\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importance\n",
    "})\n",
    "\n",
    "# Sort by importance\n",
    "importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Display the top 20 most important features\n",
    "print(\"\\nTop 20 most important features:\")\n",
    "print(importance_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d84be4",
   "metadata": {},
   "source": [
    "## Step 8: Save the model and vectorizer for future use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c90f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model\n",
    "pickle.dump(xgb_model, open(\"phishing_xgboost_model.pkl\", \"wb\"))\n",
    "\n",
    "# Save the TF-IDF vectorizer\n",
    "pickle.dump(tfidf_vectorizer, open(\"tfidf_vectorizer.pkl\", \"wb\"))\n",
    "\n",
    "print(\"Model and vectorizer saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f906f9f",
   "metadata": {},
   "source": [
    "## Step 10: Fine-tune the model (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae29ba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize grid search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False),\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit grid search (this may take time)\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Train final model with best parameters\n",
    "best_xgb_model = xgb.XGBClassifier(\n",
    "    **grid_search.best_params_,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "best_xgb_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate the tuned model\n",
    "y_pred_tuned = best_xgb_model.predict(X_test_tfidf)\n",
    "print(\"\\nTuned Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_tuned):.4f}\")\n",
    "print(classification_report(y_test, y_pred_tuned))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
