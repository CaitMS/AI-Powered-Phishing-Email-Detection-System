{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4be8abe",
   "metadata": {},
   "source": [
    "# AI-Powered Phishing Email Detection System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac324c0",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Phishing attacks are among the most prevalent forms of cyber threats, often relying on deceptive emails to trick recipients into revealing sensitive information or clicking malicious links. Traditional rule-based systems for phishing detection struggle to adapt to the rapidly evolving language and structure of phishing emails. As a result, artificial intelligence (AI) methodsâ€”particularly machine learningâ€”have become essential tools for building more flexible and accurate detection systems.\n",
    "\n",
    "In this project, we develop an AI-powered phishing email detection system that classifies emails as phishing or legitimate using natural language features and metadata. Our focus is on building a lightweight, interpretable prototype using the XGBoost classifier, a gradient boosting algorithm known for its performance and efficiency.\n",
    "\n",
    "We use a publicly available dataset from Kaggle that includes both phishing and legitimate emails with labeled examples. You can access the dataset here:\n",
    "[ðŸ”— Phishing Email Dataset on Kaggle](https://www.kaggle.com/datasets/naserabdullahalam/phishing-email-dataset)\n",
    "\n",
    "The project involves the following core components:\n",
    "*   Data cleaning and feature extraction from email content.\n",
    "*   Training and evaluation of an XGBoost classification model.\n",
    "*   Applying explainability techniques (e.g., SHAP) to interpret model predictions.\n",
    "*   Testing the model on real-world-like examples and documenting its strengths and limitations.\n",
    "\n",
    "The goal is to create a simple, explainable, and effective prototype that could form the basis of a real-world email threat detection tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83516cff",
   "metadata": {},
   "source": [
    "## Step 1: Environment set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc52b9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1ed3fb",
   "metadata": {},
   "source": [
    "## Step 2: Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a4cce34",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Where 'text_combined' contains the text data and 'target' is 1 for phishing, 0 for legitimate\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df = \u001b[43mpd\u001b[49m.read_csv(\u001b[33m'\u001b[39m\u001b[33m../dataset/phishing_email.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Check the dataset\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(df.head())\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Where 'text_combined' contains the text data and 'target' is 1 for phishing, 0 for legitimate\n",
    "\n",
    "df = pd.read_csv('../dataset/phishing_email.csv')\n",
    "\n",
    "# Check the dataset\n",
    "print(df.head())\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Class distribution:\\n{df['target'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793367b0",
   "metadata": {},
   "source": [
    "## Step 3: Split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1c58e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text_combined']\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Testing set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf1f8d4",
   "metadata": {},
   "source": [
    "## Step 4: Create TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4538394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,  # Limit features to avoid dimensionality issues\n",
    "    min_df=5,           # Ignore terms that appear in less than 5 documents\n",
    "    max_df=0.7,         # Ignore terms that appear in more than 70% of documents\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2)  # Use both unigrams and bigrams\n",
    ")\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using the fitted vectorizer\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"TF-IDF feature matrix shape: {X_train_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc7e708",
   "metadata": {},
   "source": [
    "## Step 5: Train the XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf19336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize XGBoost classifier\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(\n",
    "    X_train_tfidf, \n",
    "    y_train,\n",
    "    eval_set=[(X_test_tfidf, y_test)],\n",
    "    early_stopping_rounds=10,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"Model training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9509d9c",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4893f4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = xgb_model.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Print detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd49d0d7",
   "metadata": {},
   "source": [
    "## Step 7: Check feature importance (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41e919c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importance = xgb_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to better visualize feature importance\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importance\n",
    "})\n",
    "\n",
    "# Sort by importance\n",
    "importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Display the top 20 most important features\n",
    "print(\"\\nTop 20 most important features:\")\n",
    "print(importance_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d84be4",
   "metadata": {},
   "source": [
    "## Step 8: Save the model and vectorizer for future use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c90f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model\n",
    "pickle.dump(xgb_model, open(\"phishing_xgboost_model.pkl\", \"wb\"))\n",
    "\n",
    "# Save the TF-IDF vectorizer\n",
    "pickle.dump(tfidf_vectorizer, open(\"tfidf_vectorizer.pkl\", \"wb\"))\n",
    "\n",
    "print(\"Model and vectorizer saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f906f9f",
   "metadata": {},
   "source": [
    "## Step 10: Fine-tune the model (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae29ba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize grid search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False),\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit grid search (this may take time)\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Train final model with best parameters\n",
    "best_xgb_model = xgb.XGBClassifier(\n",
    "    **grid_search.best_params_,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "best_xgb_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate the tuned model\n",
    "y_pred_tuned = best_xgb_model.predict(X_test_tfidf)\n",
    "print(\"\\nTuned Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_tuned):.4f}\")\n",
    "print(classification_report(y_test, y_pred_tuned))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
